{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Neural-Network_Python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCMy-kOhLCuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "# Define load dataset functions\n",
        "# 定义加载数据集函数\n",
        "\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('You should use Python 3.x')\n",
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "# dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "dataset_dir = \"./datasets\"\n",
        "if not os.path.isdir(dataset_dir):\n",
        "    os.mkdir(dataset_dir)\n",
        "    \n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print(\"Downloading \" + file_name + \" ... \")\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    print(\"Done\")\n",
        "    \n",
        "def download_mnist():\n",
        "    for v in key_file.values():\n",
        "        _download(v)\n",
        "        \n",
        "def _load_label(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, img_size)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return data\n",
        "    \n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    download_mnist()\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "        \n",
        "    return T\n",
        "    \n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"读入MNIST数据集\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : 将图像的像素值正规化为0.0~1.0\n",
        "    one_hot_label : \n",
        "        one_hot_label为True的情况下，标签作为one-hot数组返回\n",
        "        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
        "    flatten : 是否将图像展开为一维数组\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (训练图像, 训练标签), (测试图像, 测试标签)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "        \n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    \n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "            \n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "    \n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYpr0R5ELVm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "# Define common functions and classes.\n",
        "# 定义常用的函数和类\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 溢出对策\n",
        "    return np.exp(x) / np.sum(np.exp(x))   \n",
        "\n",
        "# 交叉商\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # If label is one-hot-vector,convert to the correct label.\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "# 均方误差\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "    \n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val \n",
        "        it.iternext()   \n",
        "        \n",
        "    return grad    \n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "    \n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "    \n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W =W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # Derivatives of weights and parameters.\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape) \n",
        "        return dx\n",
        "\n",
        "       \n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # softmax output\n",
        "        self.t = None # label\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # label: one-hot-vector\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHZ6MsDJLjrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "# Define Network\n",
        "# 定义网络\n",
        "\n",
        "import sys, os\n",
        "sys.path.append(os.pardir) \n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class Network:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size, weight_init_std=0.01):\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size_1)\n",
        "        self.params['b1'] = np.zeros(hidden_size_1)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size_1, hidden_size_2)\n",
        "        self.params['b2'] = np.zeros(hidden_size_2)\n",
        "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size_2, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "        \n",
        "        # Generate layers.\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])        \n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    # Predict\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x:Input, t: Label\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    # Accuracy     \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # Gradient\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "            \n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
        "        \n",
        "        return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxOc9ZlzLn5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "7b866654-2729-4d04-a160-3ac9cb0d9227"
      },
      "source": [
        "# coding: utf-8\n",
        "# Training Network\n",
        "# 训练网络\n",
        "\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "# from dataset.mnist import load_mnist\n",
        "\n",
        "# Load Data\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = Network(input_size=784, hidden_size_1=50, hidden_size_2=50, output_size=10)\n",
        "\n",
        "# Hyperparameter\n",
        "iters_num = 14400   \n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "\n",
        "# Average number of repetitions per epoch\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # Calculating the gradient\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # Update parameters\n",
        "    for key in ('W1', 'b1', 'W2', 'b2','W3', 'b3'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # Record learning process \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    # Calculate the recognition accuracy of each epoch    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print('epochs: %s, train_acc: %.4f, test_acc: %.4f,'% (len(train_acc_list), train_acc, test_acc) )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "epochs: 1, train_acc: 0.1124, test_acc: 0.1135,\n",
            "epochs: 2, train_acc: 0.7393, test_acc: 0.7480,\n",
            "epochs: 3, train_acc: 0.8690, test_acc: 0.8730,\n",
            "epochs: 4, train_acc: 0.9195, test_acc: 0.9189,\n",
            "epochs: 5, train_acc: 0.9297, test_acc: 0.9241,\n",
            "epochs: 6, train_acc: 0.9540, test_acc: 0.9505,\n",
            "epochs: 7, train_acc: 0.9610, test_acc: 0.9568,\n",
            "epochs: 8, train_acc: 0.9642, test_acc: 0.9544,\n",
            "epochs: 9, train_acc: 0.9683, test_acc: 0.9599,\n",
            "epochs: 10, train_acc: 0.9741, test_acc: 0.9668,\n",
            "epochs: 11, train_acc: 0.9759, test_acc: 0.9670,\n",
            "epochs: 12, train_acc: 0.9766, test_acc: 0.9643,\n",
            "epochs: 13, train_acc: 0.9798, test_acc: 0.9668,\n",
            "epochs: 14, train_acc: 0.9824, test_acc: 0.9693,\n",
            "epochs: 15, train_acc: 0.9821, test_acc: 0.9678,\n",
            "epochs: 16, train_acc: 0.9804, test_acc: 0.9649,\n",
            "epochs: 17, train_acc: 0.9848, test_acc: 0.9700,\n",
            "epochs: 18, train_acc: 0.9847, test_acc: 0.9687,\n",
            "epochs: 19, train_acc: 0.9829, test_acc: 0.9669,\n",
            "epochs: 20, train_acc: 0.9883, test_acc: 0.9714,\n",
            "epochs: 21, train_acc: 0.9866, test_acc: 0.9696,\n",
            "epochs: 22, train_acc: 0.9880, test_acc: 0.9711,\n",
            "epochs: 23, train_acc: 0.9891, test_acc: 0.9716,\n",
            "epochs: 24, train_acc: 0.9913, test_acc: 0.9743,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "269P0WtzLtZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a9c73804-a605-4019-d2de-568ebe8b6830"
      },
      "source": [
        "# Draw training and test curve\n",
        "# 绘制训练和测试曲线\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnEWJzCqOPM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}